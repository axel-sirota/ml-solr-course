{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import gensim\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/axelsirota/repos/ml-solr-course/1-synonyms/lab1/exercise', '/Users/axelsirota/repos/ml-solr-course', '/Users/axelsirota/.pyenv/versions/3.7.3/lib/python37.zip', '/Users/axelsirota/.pyenv/versions/3.7.3/lib/python3.7', '/Users/axelsirota/.pyenv/versions/3.7.3/lib/python3.7/lib-dynload', '', '/Users/axelsirota/repos/ml-solr-course/.venv/lib/python3.7/site-packages', '/Users/axelsirota/repos/ml-solr-course/.venv/lib/python3.7/site-packages/IPython/extensions', '/Users/axelsirota/.ipython', '/Users/axelsirota/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/211.7442.45/PyCharm.app/Contents/plugins/python/helpers/pydev', '/Users/axelsirota/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/211.7442.45/PyCharm.app/Contents/plugins/python/helpers-pro/jupyter_debug']\n",
      "[Errno 2] No such file or directory: 'PATH_OF_BASE_REPO # In the solution it will be the path to my repo. This is such that python loads al the files from the top.'\n",
      "/Users/axelsirota/repos/ml-solr-course/1-synonyms/lab1/exercise\n",
      "/Users/axelsirota/repos/ml-solr-course\n"
     ]
    }
   ],
   "source": [
    "# In case your sys.path does not contain the base repo, cd there.\n",
    "print(sys.path)\n",
    "%cd 'PATH_OF_BASE_REPO'  # In the solution it will be the path to my repo. This is such that python loads al the files from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelsirota/repos/ml-solr-course/.venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121352</td>\n",
       "      <td>define extreme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>510633</td>\n",
       "      <td>tattoo fixers how much does it cost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>674172</td>\n",
       "      <td>what is a bank transit number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570009</td>\n",
       "      <td>what are the four major groups of elements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54528</td>\n",
       "      <td>blood clots in urine after menopause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                       query\n",
       "0   121352                              define extreme\n",
       "1   510633         tattoo fixers how much does it cost\n",
       "2   674172               what is a bank transit number\n",
       "3   570009  what are the four major groups of elements\n",
       "4    54528        blood clots in urine after menopause"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's load the dataset\n",
    "path = 'dataset/docv2_train_queries.tsv'\n",
    "queries = pd.read_csv(path, sep='\\t', lineterminator='\\r', names=['query_id', 'query'])\n",
    "queries.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in queries['query'].values if type(sentence) == str and len(sentence.split(' ')) >= 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "At this point we have a list (any iterable will do) of queries that are longer than 3 words. This is normal to filter random queries. Now we must use the `Tokenizer` object to `fit` on the corpus, in order to convert each wor to an ID, and later convert such corpus of list of words into their identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/g0/jqvp4xxs5pn4cds1xb9339xm0000gn/T/ipykernel_13068/1127341480.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;31m# Use the fit_on_text method to fit the tokenizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;31m# Fill\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Before the tokenizer: {corpus[:1]}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# Use the fit_on_text method to fit the tokenizer\n",
    "None # Fill\n",
    "\n",
    "print(f'Before the tokenizer: {corpus[:1]}')\n",
    "\n",
    "#Now use the same \"trained\" tokenizer to convert the corpus from words to IDs with the text_to_sequences method\n",
    "corpus = None\n",
    "\n",
    "print(f'After the tokenizer: {corpus[:1]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1   # Size of the vocabulary, adding the UNK word.\n",
    "dim = 100  # size of the embedding to create\n",
    "window_size = 3 \n",
    "epochs=50\n",
    "batch_size = 1000  # Note that this is because the workstation doesn't have a GPU. In reallity we would like a batch size and events such that we passthrough the dataset a couple of times\n",
    "BATCH = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'First 5 corpus items are {corpus[:5]}')\n",
    "print(f'Length of corpus is {len(corpus)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the core part, defining the model. Keras provides a convenient Sequential model class to just `add` layers of any type and they will just work. Let's add an `Embedding` layer (that will map the word ids into a vector of size 100), a `Lambda` to average the words out in a sentence, and a `Dense layer` to select the best word on the other end. This is classic CBOW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add()  # Add an Embedding layer with input_dim V, output_dim to be 100, and the input_length to be twice our window\n",
    "cbow.add()  # Add a Lambda that takes a lambda function using the K.mean method to average the words. The output_shape should be (dim, ).\n",
    "cbow.add(Dense(V, activation='softmax'))  # We add a classic Dense layer to just select with a softmax the best word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model with a loss and optimizer of your liking.\n",
    "cbow.compile()\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the algorithmic part of batching the dataset and yielding the window of words and expected middle word for each bacth as a generator.\n",
    "def generate_data(corpus, window_size, V, batch_size=batch_size):\n",
    "    number_of_batches = (len(corpus) // batch_size) + 1\n",
    "    for batch in range(number_of_batches):\n",
    "        lower_end = batch*batch_size\n",
    "        upper_end = (batch+1)*batch_size if batch+1 < number_of_batches else len(corpus)\n",
    "        mini_batch_size = upper_end - lower_end\n",
    "        maxlen = window_size*2\n",
    "        X = np.zeros((mini_batch_size, maxlen))\n",
    "        Y = np.zeros((mini_batch_size, V))\n",
    "        for query_id, words in enumerate(corpus[lower_end:upper_end]):\n",
    "            L = len(words)\n",
    "            for index, word in enumerate(words):\n",
    "                contexts = []\n",
    "                labels   = []            \n",
    "                s = index - window_size\n",
    "                e = index + window_size + 1\n",
    "\n",
    "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "                labels.append(word)\n",
    "\n",
    "                x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "                y = np_utils.to_categorical(labels, V)\n",
    "                X[query_id] = x\n",
    "                Y[query_id] = y\n",
    "        yield (X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data is small, you can just generate the whole dataset and load it in memory to use the fit method\n",
    "#\n",
    "# def generate_data(corpus, window_size, V):\n",
    "#         maxlen = window_size*2\n",
    "#         X = np.zeros((len(corpus), maxlen))\n",
    "#         Y = np.zeros((len(corpus), V))\n",
    "#         for query_id, words in enumerate(corpus):\n",
    "#             L = len(words)\n",
    "#             for index, word in enumerate(words):\n",
    "#                 contexts = []\n",
    "#                 labels   = []            \n",
    "#                 s = index - window_size\n",
    "#                 e = index + window_size + 1\n",
    "\n",
    "#                 contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "#                 labels.append(word)\n",
    "\n",
    "#                 x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "#                 y = np_utils.to_categorical(labels, V)\n",
    "#                 X[query_id] = x\n",
    "#                 Y[query_id] = y\n",
    "#         return (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model():\n",
    "    if not BATCH:\n",
    "        X, Y = generate_data(corpus, window_size, V)\n",
    "        print(f'Size of X is {X.shape} and Y is {Y.shape}')\n",
    "        cbow.fit(X, Y, epochs = epochs)\n",
    "    else:\n",
    "        index = 1\n",
    "        for x, y in generate_data(corpus, window_size, V):\n",
    "            print(f'Training on Iteration: {index}')\n",
    "            index += 1\n",
    "            history = cbow.train_on_batch(x, y, reset_metrics=False, return_dict=True)\n",
    "            print(history)\n",
    "            if index > epochs:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens with the losses? Try to plot them to see what trend the follow! Could it happen that some iteration gets a bigger loss than a previous iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each word we save the weights in a standard format\n",
    "with open('./1-synonyms/lab1/vectors.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(V-1, dim))\n",
    "    vectors = cbow.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "        f.write('{} {}\\n'.format(word, str_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('../1-synonyms/lab1/vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if they make sense. You can play a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['gasoline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(negative=['apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}