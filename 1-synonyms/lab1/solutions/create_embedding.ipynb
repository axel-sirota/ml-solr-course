{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/axelsirota/repos/ml-solr-course/1-synonyms/lab1/solutions', '/Users/axelsirota/.pyenv/versions/3.7.3/lib/python37.zip', '/Users/axelsirota/.pyenv/versions/3.7.3/lib/python3.7', '/Users/axelsirota/.pyenv/versions/3.7.3/lib/python3.7/lib-dynload', '', '/Users/axelsirota/repos/ml-solr-course/.venv/lib/python3.7/site-packages', '/Users/axelsirota/repos/ml-solr-course/.venv/lib/python3.7/site-packages/IPython/extensions', '/Users/axelsirota/.ipython']\n",
      "/Users/axelsirota/repos/ml-solr-course\n"
     ]
    }
   ],
   "source": [
    "# In case your sys.path does not contain the base repo, go there.\n",
    "print(sys.path)\n",
    "%cd '/Users/axelsirota/repos/ml-solr-course'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelsirota/repos/ml-solr-course/.venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121352</td>\n",
       "      <td>define extreme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>510633</td>\n",
       "      <td>tattoo fixers how much does it cost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>674172</td>\n",
       "      <td>what is a bank transit number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570009</td>\n",
       "      <td>what are the four major groups of elements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54528</td>\n",
       "      <td>blood clots in urine after menopause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                       query\n",
       "0   121352                              define extreme\n",
       "1   510633         tattoo fixers how much does it cost\n",
       "2   674172               what is a bank transit number\n",
       "3   570009  what are the four major groups of elements\n",
       "4    54528        blood clots in urine after menopause"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'dataset/docv2_train_queries.tsv'\n",
    "queries = pd.read_csv(path, sep='\\t', lineterminator='\\r', names=['query_id', 'query'])\n",
    "queries.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in queries['query'].values if type(sentence) == str and len(sentence.split(' ')) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "dim = 100\n",
    "window_size = 3\n",
    "epochs=50\n",
    "batch_size = 1000\n",
    "BATCH = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 corpus items are [[1528, 38656, 6, 24, 9, 31, 20], [1, 2, 5, 341, 4411, 43], [1, 11, 3, 913, 329, 2058, 4, 757], [53, 4020, 7, 239, 82, 1948], [1, 2, 1090, 38657]]\n",
      "Length of corpus is 312265\n"
     ]
    }
   ],
   "source": [
    "print(f'First 5 corpus items are {corpus[:5]}')\n",
    "print(f'Length of corpus is {len(corpus)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 19:20:24.823118: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 6, 100)            7644400   \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 76444)             7720844   \n",
      "=================================================================\n",
      "Total params: 15,365,244\n",
      "Trainable params: 15,365,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V, batch_size=batch_size):\n",
    "    number_of_batches = (len(corpus) // batch_size) + 1\n",
    "    for batch in range(number_of_batches):\n",
    "        lower_end = batch*batch_size\n",
    "        upper_end = (batch+1)*batch_size if batch+1 < number_of_batches else len(corpus)\n",
    "        mini_batch_size = upper_end - lower_end\n",
    "        maxlen = window_size*2\n",
    "        X = np.zeros((mini_batch_size, maxlen))\n",
    "        Y = np.zeros((mini_batch_size, V))\n",
    "        for query_id, words in enumerate(corpus[lower_end:upper_end]):\n",
    "            L = len(words)\n",
    "            for index, word in enumerate(words):\n",
    "                contexts = []\n",
    "                labels   = []            \n",
    "                s = index - window_size\n",
    "                e = index + window_size + 1\n",
    "\n",
    "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "                labels.append(word)\n",
    "\n",
    "                x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "                y = np_utils.to_categorical(labels, V)\n",
    "                X[query_id] = x\n",
    "                Y[query_id] = y\n",
    "        yield (X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data is small, you can just generate the whole dataset and load it in memory to use the fit method\n",
    "#\n",
    "# def generate_data(corpus, window_size, V):\n",
    "#         maxlen = window_size*2\n",
    "#         X = np.zeros((len(corpus), maxlen))\n",
    "#         Y = np.zeros((len(corpus), V))\n",
    "#         for query_id, words in enumerate(corpus):\n",
    "#             L = len(words)\n",
    "#             for index, word in enumerate(words):\n",
    "#                 contexts = []\n",
    "#                 labels   = []            \n",
    "#                 s = index - window_size\n",
    "#                 e = index + window_size + 1\n",
    "\n",
    "#                 contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "#                 labels.append(word)\n",
    "\n",
    "#                 x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "#                 y = np_utils.to_categorical(labels, V)\n",
    "#                 X[query_id] = x\n",
    "#                 Y[query_id] = y\n",
    "#         return (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model():\n",
    "    if not BATCH:\n",
    "        X, Y = generate_data(corpus, window_size, V)\n",
    "        print(f'Size of X is {X.shape} and Y is {Y.shape}')\n",
    "        cbow.fit(X, Y, epochs = epochs)\n",
    "    else:\n",
    "        index = 1\n",
    "        for x, y in generate_data(corpus, window_size, V):\n",
    "            print(f'Training on Iteration: {index}')\n",
    "            index += 1\n",
    "            history = cbow.train_on_batch(x, y, reset_metrics=False, return_dict=True)\n",
    "            print(history)\n",
    "            if index > epochs:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 19:20:36.296448: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.24425983428955}\n",
      "Training on Iteration: 2\n",
      "{'loss': 11.243762016296387}\n",
      "Training on Iteration: 3\n",
      "{'loss': 11.243168830871582}\n",
      "Training on Iteration: 4\n",
      "{'loss': 11.242558479309082}\n",
      "Training on Iteration: 5\n",
      "{'loss': 11.241923332214355}\n",
      "Training on Iteration: 6\n",
      "{'loss': 11.2412691116333}\n",
      "Training on Iteration: 7\n",
      "{'loss': 11.240599632263184}\n",
      "Training on Iteration: 8\n",
      "{'loss': 11.239843368530273}\n",
      "Training on Iteration: 9\n",
      "{'loss': 11.239004135131836}\n",
      "Training on Iteration: 10\n",
      "{'loss': 11.238199234008789}\n",
      "Training on Iteration: 11\n",
      "{'loss': 11.23741340637207}\n",
      "Training on Iteration: 12\n",
      "{'loss': 11.236527442932129}\n",
      "Training on Iteration: 13\n",
      "{'loss': 11.235565185546875}\n",
      "Training on Iteration: 14\n",
      "{'loss': 11.23462200164795}\n",
      "Training on Iteration: 15\n",
      "{'loss': 11.233726501464844}\n",
      "Training on Iteration: 16\n",
      "{'loss': 11.232732772827148}\n",
      "Training on Iteration: 17\n",
      "{'loss': 11.231771469116211}\n",
      "Training on Iteration: 18\n",
      "{'loss': 11.230607032775879}\n",
      "Training on Iteration: 19\n",
      "{'loss': 11.229459762573242}\n",
      "Training on Iteration: 20\n",
      "{'loss': 11.22830867767334}\n",
      "Training on Iteration: 21\n",
      "{'loss': 11.22714900970459}\n",
      "Training on Iteration: 22\n",
      "{'loss': 11.225926399230957}\n",
      "Training on Iteration: 23\n",
      "{'loss': 11.224647521972656}\n",
      "Training on Iteration: 24\n",
      "{'loss': 11.223352432250977}\n",
      "Training on Iteration: 25\n",
      "{'loss': 11.221956253051758}\n",
      "Training on Iteration: 26\n",
      "{'loss': 11.22057819366455}\n",
      "Training on Iteration: 27\n",
      "{'loss': 11.219249725341797}\n",
      "Training on Iteration: 28\n",
      "{'loss': 11.217790603637695}\n",
      "Training on Iteration: 29\n",
      "{'loss': 11.216252326965332}\n",
      "Training on Iteration: 30\n",
      "{'loss': 11.214676856994629}\n",
      "Training on Iteration: 31\n",
      "{'loss': 11.212936401367188}\n",
      "Training on Iteration: 32\n",
      "{'loss': 11.211386680603027}\n",
      "Training on Iteration: 33\n",
      "{'loss': 11.209592819213867}\n",
      "Training on Iteration: 34\n",
      "{'loss': 11.207785606384277}\n",
      "Training on Iteration: 35\n",
      "{'loss': 11.205987930297852}\n",
      "Training on Iteration: 36\n",
      "{'loss': 11.204050064086914}\n",
      "Training on Iteration: 37\n",
      "{'loss': 11.20212459564209}\n",
      "Training on Iteration: 38\n",
      "{'loss': 11.200094223022461}\n",
      "Training on Iteration: 39\n",
      "{'loss': 11.19808578491211}\n",
      "Training on Iteration: 40\n",
      "{'loss': 11.19582462310791}\n",
      "Training on Iteration: 41\n",
      "{'loss': 11.193710327148438}\n",
      "Training on Iteration: 42\n",
      "{'loss': 11.191527366638184}\n",
      "Training on Iteration: 43\n",
      "{'loss': 11.189123153686523}\n",
      "Training on Iteration: 44\n",
      "{'loss': 11.186675071716309}\n",
      "Training on Iteration: 45\n",
      "{'loss': 11.184183120727539}\n",
      "Training on Iteration: 46\n",
      "{'loss': 11.181548118591309}\n",
      "Training on Iteration: 47\n",
      "{'loss': 11.178803443908691}\n",
      "Training on Iteration: 48\n",
      "{'loss': 11.176204681396484}\n",
      "Training on Iteration: 49\n",
      "{'loss': 11.173500061035156}\n",
      "Training on Iteration: 50\n",
      "{'loss': 11.170660018920898}\n",
      "Training on Iteration: 51\n",
      "{'loss': 11.167884826660156}\n",
      "Training on Iteration: 52\n",
      "{'loss': 11.164794921875}\n",
      "Training on Iteration: 53\n",
      "{'loss': 11.161873817443848}\n",
      "Training on Iteration: 54\n",
      "{'loss': 11.15890884399414}\n",
      "Training on Iteration: 55\n",
      "{'loss': 11.155717849731445}\n",
      "Training on Iteration: 56\n",
      "{'loss': 11.152497291564941}\n",
      "Training on Iteration: 57\n",
      "{'loss': 11.149291038513184}\n",
      "Training on Iteration: 58\n",
      "{'loss': 11.14594841003418}\n",
      "Training on Iteration: 59\n",
      "{'loss': 11.14234733581543}\n",
      "Training on Iteration: 60\n",
      "{'loss': 11.138800621032715}\n",
      "Training on Iteration: 61\n",
      "{'loss': 11.135058403015137}\n",
      "Training on Iteration: 62\n",
      "{'loss': 11.13106632232666}\n",
      "Training on Iteration: 63\n",
      "{'loss': 11.127215385437012}\n",
      "Training on Iteration: 64\n",
      "{'loss': 11.123177528381348}\n",
      "Training on Iteration: 65\n",
      "{'loss': 11.119359970092773}\n",
      "Training on Iteration: 66\n",
      "{'loss': 11.11515998840332}\n",
      "Training on Iteration: 67\n",
      "{'loss': 11.11098861694336}\n",
      "Training on Iteration: 68\n",
      "{'loss': 11.106688499450684}\n",
      "Training on Iteration: 69\n",
      "{'loss': 11.10228443145752}\n",
      "Training on Iteration: 70\n",
      "{'loss': 11.097904205322266}\n",
      "Training on Iteration: 71\n",
      "{'loss': 11.09338665008545}\n",
      "Training on Iteration: 72\n",
      "{'loss': 11.088577270507812}\n",
      "Training on Iteration: 73\n",
      "{'loss': 11.083984375}\n",
      "Training on Iteration: 74\n",
      "{'loss': 11.079170227050781}\n",
      "Training on Iteration: 75\n",
      "{'loss': 11.074077606201172}\n",
      "Training on Iteration: 76\n",
      "{'loss': 11.069046974182129}\n",
      "Training on Iteration: 77\n",
      "{'loss': 11.063676834106445}\n",
      "Training on Iteration: 78\n",
      "{'loss': 11.058871269226074}\n",
      "Training on Iteration: 79\n",
      "{'loss': 11.05359935760498}\n",
      "Training on Iteration: 80\n",
      "{'loss': 11.048199653625488}\n",
      "Training on Iteration: 81\n",
      "{'loss': 11.042745590209961}\n",
      "Training on Iteration: 82\n",
      "{'loss': 11.037052154541016}\n",
      "Training on Iteration: 83\n",
      "{'loss': 11.031266212463379}\n",
      "Training on Iteration: 84\n",
      "{'loss': 11.025826454162598}\n",
      "Training on Iteration: 85\n",
      "{'loss': 11.019803047180176}\n",
      "Training on Iteration: 86\n",
      "{'loss': 11.014084815979004}\n",
      "Training on Iteration: 87\n",
      "{'loss': 11.007833480834961}\n",
      "Training on Iteration: 88\n",
      "{'loss': 11.001827239990234}\n",
      "Training on Iteration: 89\n",
      "{'loss': 10.99584674835205}\n",
      "Training on Iteration: 90\n",
      "{'loss': 10.98940372467041}\n",
      "Training on Iteration: 91\n",
      "{'loss': 10.983572959899902}\n",
      "Training on Iteration: 92\n",
      "{'loss': 10.976935386657715}\n",
      "Training on Iteration: 93\n",
      "{'loss': 10.97064208984375}\n",
      "Training on Iteration: 94\n",
      "{'loss': 10.964117050170898}\n",
      "Training on Iteration: 95\n",
      "{'loss': 10.95726203918457}\n",
      "Training on Iteration: 96\n",
      "{'loss': 10.950573921203613}\n",
      "Training on Iteration: 97\n",
      "{'loss': 10.9437255859375}\n",
      "Training on Iteration: 98\n",
      "{'loss': 10.936982154846191}\n",
      "Training on Iteration: 99\n",
      "{'loss': 10.929892539978027}\n",
      "Training on Iteration: 100\n",
      "{'loss': 10.922922134399414}\n",
      "Training on Iteration: 101\n",
      "{'loss': 10.915657043457031}\n",
      "Training on Iteration: 102\n",
      "{'loss': 10.908076286315918}\n",
      "Training on Iteration: 103\n",
      "{'loss': 10.900308609008789}\n",
      "Training on Iteration: 104\n",
      "{'loss': 10.893227577209473}\n",
      "Training on Iteration: 105\n",
      "{'loss': 10.885091781616211}\n",
      "Training on Iteration: 106\n",
      "{'loss': 10.87744140625}\n",
      "Training on Iteration: 107\n",
      "{'loss': 10.870142936706543}\n",
      "Training on Iteration: 108\n",
      "{'loss': 10.8623046875}\n",
      "Training on Iteration: 109\n",
      "{'loss': 10.854972839355469}\n",
      "Training on Iteration: 110\n",
      "{'loss': 10.847440719604492}\n",
      "Training on Iteration: 111\n",
      "{'loss': 10.83979606628418}\n",
      "Training on Iteration: 112\n",
      "{'loss': 10.831993103027344}\n",
      "Training on Iteration: 113\n",
      "{'loss': 10.823627471923828}\n",
      "Training on Iteration: 114\n",
      "{'loss': 10.815543174743652}\n",
      "Training on Iteration: 115\n",
      "{'loss': 10.80759048461914}\n",
      "Training on Iteration: 116\n",
      "{'loss': 10.799219131469727}\n",
      "Training on Iteration: 117\n",
      "{'loss': 10.791266441345215}\n",
      "Training on Iteration: 118\n",
      "{'loss': 10.782586097717285}\n",
      "Training on Iteration: 119\n",
      "{'loss': 10.774412155151367}\n",
      "Training on Iteration: 120\n",
      "{'loss': 10.765756607055664}\n",
      "Training on Iteration: 121\n",
      "{'loss': 10.75713062286377}\n",
      "Training on Iteration: 122\n",
      "{'loss': 10.748650550842285}\n",
      "Training on Iteration: 123\n",
      "{'loss': 10.740334510803223}\n",
      "Training on Iteration: 124\n",
      "{'loss': 10.731674194335938}\n",
      "Training on Iteration: 125\n",
      "{'loss': 10.723457336425781}\n",
      "Training on Iteration: 126\n",
      "{'loss': 10.714670181274414}\n",
      "Training on Iteration: 127\n",
      "{'loss': 10.705406188964844}\n",
      "Training on Iteration: 128\n",
      "{'loss': 10.696599960327148}\n",
      "Training on Iteration: 129\n",
      "{'loss': 10.687193870544434}\n",
      "Training on Iteration: 130\n",
      "{'loss': 10.67848014831543}\n",
      "Training on Iteration: 131\n",
      "{'loss': 10.669583320617676}\n",
      "Training on Iteration: 132\n",
      "{'loss': 10.660093307495117}\n",
      "Training on Iteration: 133\n",
      "{'loss': 10.651110649108887}\n",
      "Training on Iteration: 134\n",
      "{'loss': 10.642986297607422}\n",
      "Training on Iteration: 135\n",
      "{'loss': 10.633987426757812}\n",
      "Training on Iteration: 136\n",
      "{'loss': 10.625157356262207}\n",
      "Training on Iteration: 137\n",
      "{'loss': 10.616402626037598}\n",
      "Training on Iteration: 138\n",
      "{'loss': 10.607096672058105}\n",
      "Training on Iteration: 139\n",
      "{'loss': 10.598381996154785}\n",
      "Training on Iteration: 140\n",
      "{'loss': 10.588642120361328}\n",
      "Training on Iteration: 141\n",
      "{'loss': 10.579862594604492}\n",
      "Training on Iteration: 142\n",
      "{'loss': 10.571249961853027}\n",
      "Training on Iteration: 143\n",
      "{'loss': 10.562407493591309}\n",
      "Training on Iteration: 144\n",
      "{'loss': 10.55354118347168}\n",
      "Training on Iteration: 145\n",
      "{'loss': 10.54444694519043}\n",
      "Training on Iteration: 146\n",
      "{'loss': 10.53624153137207}\n",
      "Training on Iteration: 147\n",
      "{'loss': 10.527393341064453}\n",
      "Training on Iteration: 148\n",
      "{'loss': 10.518899917602539}\n",
      "Training on Iteration: 149\n",
      "{'loss': 10.510199546813965}\n",
      "Training on Iteration: 150\n",
      "{'loss': 10.501607894897461}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Iteration: 151\n",
      "{'loss': 10.492704391479492}\n",
      "Training on Iteration: 152\n",
      "{'loss': 10.483600616455078}\n",
      "Training on Iteration: 153\n",
      "{'loss': 10.47494888305664}\n",
      "Training on Iteration: 154\n",
      "{'loss': 10.466867446899414}\n",
      "Training on Iteration: 155\n",
      "{'loss': 10.458367347717285}\n",
      "Training on Iteration: 156\n",
      "{'loss': 10.449788093566895}\n",
      "Training on Iteration: 157\n",
      "{'loss': 10.441794395446777}\n",
      "Training on Iteration: 158\n",
      "{'loss': 10.432795524597168}\n",
      "Training on Iteration: 159\n",
      "{'loss': 10.424227714538574}\n",
      "Training on Iteration: 160\n",
      "{'loss': 10.415639877319336}\n",
      "Training on Iteration: 161\n",
      "{'loss': 10.406964302062988}\n",
      "Training on Iteration: 162\n",
      "{'loss': 10.39923095703125}\n",
      "Training on Iteration: 163\n",
      "{'loss': 10.390838623046875}\n",
      "Training on Iteration: 164\n",
      "{'loss': 10.382746696472168}\n",
      "Training on Iteration: 165\n",
      "{'loss': 10.374244689941406}\n",
      "Training on Iteration: 166\n",
      "{'loss': 10.36579418182373}\n",
      "Training on Iteration: 167\n",
      "{'loss': 10.357924461364746}\n",
      "Training on Iteration: 168\n",
      "{'loss': 10.34937858581543}\n",
      "Training on Iteration: 169\n",
      "{'loss': 10.341294288635254}\n",
      "Training on Iteration: 170\n",
      "{'loss': 10.333608627319336}\n",
      "Training on Iteration: 171\n",
      "{'loss': 10.325972557067871}\n",
      "Training on Iteration: 172\n",
      "{'loss': 10.31747055053711}\n",
      "Training on Iteration: 173\n",
      "{'loss': 10.309825897216797}\n",
      "Training on Iteration: 174\n",
      "{'loss': 10.30184268951416}\n",
      "Training on Iteration: 175\n",
      "{'loss': 10.2938871383667}\n",
      "Training on Iteration: 176\n",
      "{'loss': 10.285778045654297}\n",
      "Training on Iteration: 177\n",
      "{'loss': 10.278450965881348}\n",
      "Training on Iteration: 178\n"
     ]
    }
   ],
   "source": [
    "fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./1-synonyms/lab1/vectors.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(V-1, dim))\n",
    "    vectors = cbow.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "        f.write('{} {}\\n'.format(word, str_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./1-synonyms/lab1/vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['gasoline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(negative=['apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}