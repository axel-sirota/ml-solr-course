{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.initializers import Constant\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/axelsirota/repos/ml-solr-course/1-synonyms/lab2/solutions', '/Users/axelsirota/.pyenv/versions/3.7.3/lib/python37.zip', '/Users/axelsirota/.pyenv/versions/3.7.3/lib/python3.7', '/Users/axelsirota/.pyenv/versions/3.7.3/lib/python3.7/lib-dynload', '', '/Users/axelsirota/repos/ml-solr-course/.venv/lib/python3.7/site-packages', '/Users/axelsirota/repos/ml-solr-course/.venv/lib/python3.7/site-packages/IPython/extensions', '/Users/axelsirota/.ipython']\n",
      "/Users/axelsirota/repos/ml-solr-course\n"
     ]
    }
   ],
   "source": [
    "# In case your sys.path does not contain the base repo, go there.\n",
    "print(sys.path)\n",
    "%cd '/Users/axelsirota/repos/ml-solr-course'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelsirota/repos/ml-solr-course/.venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121352</td>\n",
       "      <td>define extreme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>510633</td>\n",
       "      <td>tattoo fixers how much does it cost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>674172</td>\n",
       "      <td>what is a bank transit number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570009</td>\n",
       "      <td>what are the four major groups of elements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54528</td>\n",
       "      <td>blood clots in urine after menopause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                       query\n",
       "0   121352                              define extreme\n",
       "1   510633         tattoo fixers how much does it cost\n",
       "2   674172               what is a bank transit number\n",
       "3   570009  what are the four major groups of elements\n",
       "4    54528        blood clots in urine after menopause"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'dataset/docv2_train_queries.tsv'\n",
    "queries = pd.read_csv(path, sep='\\t', lineterminator='\\r', names=['query_id', 'query'])[:20000]\n",
    "queries.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in queries['query'].values if type(sentence) == str and len(sentence.split(' ')) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = \"./dataset/glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "dim = 100\n",
    "window_size = 3\n",
    "epochs=50\n",
    "batch_size = 5000\n",
    "BATCH = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 corpus items are [[1129, 7060, 6, 23, 9, 29, 21], [1, 2, 5, 296, 3382, 40], [1, 11, 3, 613, 371, 2691, 4, 614], [56, 2231, 7, 199, 89, 927], [1, 2, 1432, 7061]]\n",
      "Length of corpus is 19403\n"
     ]
    }
   ],
   "source": [
    "print(f'First 5 corpus items are {corpus[:5]}')\n",
    "print(f'Length of corpus is {len(corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15878 words (1324 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = V + 1\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 19:29:22.210476: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=num_tokens, output_dim=dim, input_length=window_size*2, embeddings_initializer=Constant(embedding_matrix),\n",
    "    trainable=False))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 6, 100)            1720400   \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 17203)             1737503   \n",
      "=================================================================\n",
      "Total params: 3,457,903\n",
      "Trainable params: 1,737,503\n",
      "Non-trainable params: 1,720,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V, batch_size=batch_size):\n",
    "    number_of_batches = (len(corpus) // batch_size) + 1\n",
    "    for batch in range(number_of_batches):\n",
    "        lower_end = batch*batch_size\n",
    "        upper_end = (batch+1)*batch_size if batch+1 < number_of_batches else len(corpus)\n",
    "        mini_batch_size = upper_end - lower_end\n",
    "        maxlen = window_size*2\n",
    "        X = np.zeros((mini_batch_size, maxlen))\n",
    "        Y = np.zeros((mini_batch_size, V))\n",
    "        for query_id, words in enumerate(corpus[lower_end:upper_end]):\n",
    "            L = len(words)\n",
    "            for index, word in enumerate(words):\n",
    "                contexts = []\n",
    "                labels   = []            \n",
    "                s = index - window_size\n",
    "                e = index + window_size + 1\n",
    "\n",
    "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "                labels.append(word)\n",
    "\n",
    "                x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "                y = np_utils.to_categorical(labels, V)\n",
    "                X[query_id] = x\n",
    "                Y[query_id] = y\n",
    "        yield (X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data is small, you can just generate the whole dataset and load it in memory to use the fit method\n",
    "#\n",
    "if not BATCH:\n",
    "    def generate_data(corpus, window_size, V):\n",
    "            maxlen = window_size*2\n",
    "            X = np.zeros((len(corpus), maxlen))\n",
    "            Y = np.zeros((len(corpus), V))\n",
    "            for query_id, words in enumerate(corpus):\n",
    "                L = len(words)\n",
    "                for index, word in enumerate(words):\n",
    "                    contexts = []\n",
    "                    labels   = []            \n",
    "                    s = index - window_size\n",
    "                    e = index + window_size + 1\n",
    "\n",
    "                    contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "                    labels.append(word)\n",
    "\n",
    "                    x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "                    y = np_utils.to_categorical(labels, V)\n",
    "                    X[query_id] = x\n",
    "                    Y[query_id] = y\n",
    "            return (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model():\n",
    "    if not BATCH:\n",
    "        X, Y = generate_data(corpus, window_size, V)\n",
    "        print(f'Size of X is {X.shape} and Y is {Y.shape}')\n",
    "        cbow.fit(X, Y, epochs = epochs)\n",
    "    else:\n",
    "        index = 1\n",
    "        for x, y in generate_data(corpus, window_size, V):\n",
    "            print(f'Training on Iteration: {index}')\n",
    "            index += 1\n",
    "            history = cbow.train_on_batch(x, y, reset_metrics=False, return_dict=True)\n",
    "            print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X is (19403, 6) and Y is (19403, 17203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 19:29:29.200362: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "607/607 [==============================] - 15s 9ms/step - loss: 9.4008\n",
      "Epoch 2/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 8.2248\n",
      "Epoch 3/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 7.8407\n",
      "Epoch 4/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 7.5275\n",
      "Epoch 5/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 7.3134\n",
      "Epoch 6/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 7.1092\n",
      "Epoch 7/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 6.8781\n",
      "Epoch 8/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 6.7523\n",
      "Epoch 9/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 6.5830\n",
      "Epoch 10/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 6.4178\n",
      "Epoch 11/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 6.2461\n",
      "Epoch 12/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 6.1194\n",
      "Epoch 13/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 6.0048\n",
      "Epoch 14/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 5.8431\n",
      "Epoch 15/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 5.7331\n",
      "Epoch 16/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 5.6128\n",
      "Epoch 17/50\n",
      "607/607 [==============================] - 6s 10ms/step - loss: 5.4838\n",
      "Epoch 18/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 5.3908\n",
      "Epoch 19/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 5.2821\n",
      "Epoch 20/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 5.1904\n",
      "Epoch 21/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 5.0779\n",
      "Epoch 22/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 4.9959\n",
      "Epoch 23/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 4.9127\n",
      "Epoch 24/50\n",
      "607/607 [==============================] - 5s 9ms/step - loss: 4.8078\n",
      "Epoch 25/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 4.6960\n",
      "Epoch 26/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 4.6119\n",
      "Epoch 27/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 4.5643\n",
      "Epoch 28/50\n",
      "607/607 [==============================] - 5s 9ms/step - loss: 4.4798\n",
      "Epoch 29/50\n",
      "607/607 [==============================] - 5s 9ms/step - loss: 4.3885\n",
      "Epoch 30/50\n",
      "607/607 [==============================] - 5s 9ms/step - loss: 4.3268\n",
      "Epoch 31/50\n",
      "607/607 [==============================] - 5s 9ms/step - loss: 4.2375\n",
      "Epoch 32/50\n",
      "607/607 [==============================] - 5s 9ms/step - loss: 4.2130\n",
      "Epoch 33/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 4.1292\n",
      "Epoch 34/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 4.0402\n",
      "Epoch 35/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.9865\n",
      "Epoch 36/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.9238\n",
      "Epoch 37/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.8252\n",
      "Epoch 38/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.7915\n",
      "Epoch 39/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.7428\n",
      "Epoch 40/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.6753\n",
      "Epoch 41/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.6468\n",
      "Epoch 42/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.5867\n",
      "Epoch 43/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.5322\n",
      "Epoch 44/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.4875\n",
      "Epoch 45/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.4523\n",
      "Epoch 46/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.3685\n",
      "Epoch 47/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.3255\n",
      "Epoch 48/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.2979\n",
      "Epoch 49/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.2821\n",
      "Epoch 50/50\n",
      "607/607 [==============================] - 6s 9ms/step - loss: 3.2178\n"
     ]
    }
   ],
   "source": [
    "fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./1-synonyms/lab2/vectors.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(V-1, dim))\n",
    "    vectors = cbow.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "        f.write('{} {}\\n'.format(word, str_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./1-synonyms/lab2/vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelsirota/repos/ml-solr-course/.venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py:772: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('petrol', 0.8148176670074463),\n",
       " ('fuel', 0.8057637214660645),\n",
       " ('heating', 0.7722175717353821),\n",
       " ('crude', 0.7561777234077454),\n",
       " ('diesel', 0.755372941493988),\n",
       " ('prices', 0.7397462725639343),\n",
       " ('gallon', 0.7387636303901672),\n",
       " ('gas', 0.7218859195709229),\n",
       " ('fuels', 0.6957605481147766),\n",
       " ('oil', 0.6737465858459473)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['gasoline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('grapes', 0.7915149927139282),\n",
       " ('wine', 0.7132691144943237),\n",
       " ('varieties', 0.7115378975868225),\n",
       " ('chardonnay', 0.7091464996337891),\n",
       " ('pinot', 0.7055837512016296),\n",
       " ('vines', 0.681800127029419),\n",
       " ('vine', 0.6526992917060852),\n",
       " ('fruit', 0.6477821469306946),\n",
       " ('tomato', 0.6304808855056763),\n",
       " ('citrus', 0.6102907657623291)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['grape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}