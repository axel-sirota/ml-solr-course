{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.initializers import Constant\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case your sys.path does not contain the base repo, cd there.\n",
    "print(sys.path)\n",
    "%cd 'PATH_OF_BASE_REPO'  # In the solution it will be the path to my repo. This is such that python loads al the files from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'dataset/docv2_train_queries.tsv'\n",
    "queries = pd.read_csv(path, sep='\\t', lineterminator='\\r', names=['query_id', 'query'])[:20000]\n",
    "queries.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in queries['query'].values if type(sentence) == str and len(sentence.split(' ')) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the pretrained embedding\n",
    "path_to_glove_file = \"./dataset/glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "dim = 100\n",
    "window_size = 3\n",
    "epochs=50\n",
    "batch_size = 5000\n",
    "BATCH = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'First 5 corpus items are {corpus[:5]}')\n",
    "print(f'Length of corpus is {len(corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the interesting part, we need to construct a matrix of `V+1 x dim` and for each word in the tokenizer, try to get it from the embedding. If it doesn't exist then just fill it with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_tokens = V + 1\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = None\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = None  # Get the embedding vector from the GloVe embedding\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "cbow = Sequential()\n",
    "cbow.add()  # Add the same Embedding as before, but the embeddings initializer will be the embedding matrix we have built, and trainable to False. This way we start from the pretrained embedding.\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the Non-trainable parameters! What we are doing is just training the softmax based on correct embeddings. This is called fine tuning the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V, batch_size=batch_size):\n",
    "    number_of_batches = (len(corpus) // batch_size) + 1\n",
    "    for batch in range(number_of_batches):\n",
    "        lower_end = batch*batch_size\n",
    "        upper_end = (batch+1)*batch_size if batch+1 < number_of_batches else len(corpus)\n",
    "        mini_batch_size = upper_end - lower_end\n",
    "        maxlen = window_size*2\n",
    "        X = np.zeros((mini_batch_size, maxlen))\n",
    "        Y = np.zeros((mini_batch_size, V))\n",
    "        for query_id, words in enumerate(corpus[lower_end:upper_end]):\n",
    "            L = len(words)\n",
    "            for index, word in enumerate(words):\n",
    "                contexts = []\n",
    "                labels   = []            \n",
    "                s = index - window_size\n",
    "                e = index + window_size + 1\n",
    "\n",
    "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "                labels.append(word)\n",
    "\n",
    "                x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "                y = np_utils.to_categorical(labels, V)\n",
    "                X[query_id] = x\n",
    "                Y[query_id] = y\n",
    "        yield (X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data is small, you can just generate the whole dataset and load it in memory to use the fit method\n",
    "#\n",
    "if not BATCH:\n",
    "    def generate_data(corpus, window_size, V):\n",
    "            maxlen = window_size*2\n",
    "            X = np.zeros((len(corpus), maxlen))\n",
    "            Y = np.zeros((len(corpus), V))\n",
    "            for query_id, words in enumerate(corpus):\n",
    "                L = len(words)\n",
    "                for index, word in enumerate(words):\n",
    "                    contexts = []\n",
    "                    labels   = []            \n",
    "                    s = index - window_size\n",
    "                    e = index + window_size + 1\n",
    "\n",
    "                    contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "                    labels.append(word)\n",
    "\n",
    "                    x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "                    y = np_utils.to_categorical(labels, V)\n",
    "                    X[query_id] = x\n",
    "                    Y[query_id] = y\n",
    "            return (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model():\n",
    "    if not BATCH:\n",
    "        X, Y = generate_data(corpus, window_size, V)\n",
    "        print(f'Size of X is {X.shape} and Y is {Y.shape}')\n",
    "        cbow.fit(X, Y, epochs = epochs)\n",
    "    else:\n",
    "        index = 1\n",
    "        for x, y in generate_data(corpus, window_size, V):\n",
    "            print(f'Training on Iteration: {index}')\n",
    "            index += 1\n",
    "            history = cbow.train_on_batch(x, y, reset_metrics=False, return_dict=True)\n",
    "            print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./1-synonyms/lab2/vectors.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(V-1, dim))\n",
    "    vectors = cbow.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "        f.write('{} {}\\n'.format(word, str_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./1-synonyms/lab2/vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['gasoline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['grape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice the difference in the accuracy? For any task first search if there are any pretrained models to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
