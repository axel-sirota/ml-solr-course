{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pysolr\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "np.random.seed(42)\n",
    "import smart_open\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case your sys.path does not contain the base repo, cd there.\n",
    "print(sys.path)\n",
    "%cd 'PATH_OF_BASE_REPO'  # In the solution it will be the path to my repo. This is such that python loads al the files from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_path = '2-ranking/lab4/airbnb_model'\n",
    "query = 'Midtown sunny chateau'\n",
    "number_of_initial_retrieved = 100\n",
    "model = None # Load the Doc2Vec model from lab4\n",
    "print(f'Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Instantiate the client\n",
    "solr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Search for the query\n",
    "results = []\n",
    "print(f'Number of results were {len(results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use the same simple_preprocess from the last lab 4 to tokenize the query\n",
    "tokenized_query = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inferred_vector = model.infer_vector(tokenized_query)\n",
    "print(inferred_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "similarities = []\n",
    "for result in results:\n",
    "    similarity = 0 # Find the similarity between the query and the result using gensim similarity_unseen_docs method\n",
    "    similarities.append(similarity)\n",
    "df_results[\"Similarity\"] = pd.Series(similarities)  # We store the similarities to order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ok, what we have done is do the query against Solr, then finding the similarity between the descriptions of the query and the results.\n",
    "\n",
    "The idea behind the algorithm would be to reorder the results based on the similarity score, not on BM25. Let's see which one is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = None # Sort the df_results by similarity column in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = a[:10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Most similar document after reranking within retrieved results has description: \\n\\n{a[\"description\"].iloc[0]}\\nWith similarity: {a[\"Similarity\"].iloc[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Most similar document before reranking within retrieved results has description: \\n\\n{df_results[\"description\"].iloc[0]}\\nWith similarity: {df_results[\"Similarity\"].iloc[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of documents that surpass 0.5 similarity threshold: {len(a[a[\"Similarity\"] >= 0.5])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is remarkable how using DBOW the most similar result understood the need for midtown apartments that are chateaus. On the other hand the traditional top result ponderated chateau more just because it is a rare word.\n",
    "\n",
    "It is not a perfect method, but a very good indication. A good idea is to have something like this **between** the raw results (thousands), filter them by similarity (hundreds) and then have a learning to rank recommender (dozens).\n",
    "\n",
    "Tensorflow has opensources TF Recommenders which is great to plug in as an algorithm **after** these results. But this alone would work just fine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
