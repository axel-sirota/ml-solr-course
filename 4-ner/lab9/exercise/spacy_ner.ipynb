{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pysolr\n",
    "\n",
    "np.random.seed(42)\n",
    "import spacy\n",
    "import sys\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# In case your sys.path does not contain the base repo, cd there.\n",
    "print(sys.path)\n",
    "%cd '~/ml-solr-course'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nlp = None # Load the en_core_web_trf from spacy. This is a transformer model like GPT-2\n",
    "dataset = pd.read_csv('dataset/new_york_reduced.csv')[:100][[\"id\", \"name\", \"description\", \"neighbourhood_cleansed\", \"property_type\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "first_description = dataset[\"description\"].iloc[0]\n",
    "first_description\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the nlp method in the first)_description to see what it returns\n",
    "doc = None\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each document will have one or many entities. Each having a specific type (LOCATION, ORG, DATE, NUMBER, GPE, etc...) a starting character, and an ending character to define where it is.\n",
    "\n",
    "If we have a field that is too expensive to store or index, we could only index their main named entities by enriching the index with these fields at index time, and indexing those alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tags = []\n",
    "\n",
    "# We will create a function that will get the descriptions one by one and append the tags to the tags cache\n",
    "def update_caches(document):\n",
    "    doc = None  # Run nlp on the document\n",
    "    inner_tags = []\n",
    "    # Append each entity text property to the inner_tags list\n",
    "    tags_to_append = inner_tags if len(inner_tags) else None\n",
    "    tags.append(tags_to_append)\n",
    "\n",
    "update_caches = np.vectorize(update_caches)  # This is for speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "_ = update_caches(dataset[[\"description\"]].values)  # -> As you can tell this takes a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A good strategy while we wait, is notice that NER is both hard and slow, so is best to apply and enrich while we index, since that process already takes time.\n",
    "\n",
    "Or find a vectorized implementation of spacy's nlp (I haven't found any) or find if we can cythonize it (haven't tried honestly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset, pd.Series(tags, name=\"tags\")], axis=1)\n",
    "dataset[\"id\"] = pd.to_numeric(dataset[\"id\"], downcast='integer')\n",
    "dataset = dataset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the enriched dataset to index it in a new core.\n",
    "dataset.to_csv(\"./4-ner/lab9/expanded_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}